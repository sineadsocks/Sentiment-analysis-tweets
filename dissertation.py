# -*- coding: utf-8 -*-
"""dissertation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OuK8MJDBDlND7Il_E2rthUBxP9IuVKWU
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from google.colab import files

import nltk
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')

from wordcloud import  WordCloud, STOPWORDS, ImageColorGenerator

from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score

from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler

"""##Reading in data"""

files.upload()

EM_tweets = pd.read_csv('Elon Musk.csv')

pd.set_option('display.max_colwidth', None)

EM_tweets.head()

features = EM_tweets.iloc[:, 1].values
labels = EM_tweets.iloc[:, 2].values
y = 1-pd.factorize(labels)[0] #make positive =1 and negative =0

y

data = {'features': features,
        'labels': y}

EM_df = pd.DataFrame(data, columns=['features', 'labels'])

EM_df.head()

"""##Data Analysis"""

EM_df['length'] = EM_df['features'].apply(len)
fig1 = sns.barplot('labels','length',data = EM_df,palette='PRGn')
plt.title('Average Tweet Length vs label')
plot = fig1.get_figure()
plot.savefig('Barplot.png')

EM_tweets['sentiment'].value_counts()

EM_tweets.sentiment.value_counts().plot(kind='pie', autopct='%.1f', colors=['pink', 'cornflowerblue'])



"""##Data preprocessing and lemmatization"""

#before processing
features[0]

processed_features = []

for sentence in range(0,len(features)):

  processed_feature = re.sub('@[^\s]+', ' ', str(features[sentence]))  #remove usernames

  processed_feature = re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature) #remove single characters

  processed_feature = re.sub(r"\^[a-zA-Z ]\s+", ' ', processed_feature) # remove single charactres from start

  processed_feature = re.sub(r'\s+', ' ', processed_feature, flags = re.I) #make multiple spaces into single space

  processed_feature = re.sub(r'http\S+',' ', processed_feature)  #remove http/s links

  processed_feature = re.sub(r"[^a-zA-Z\s\(\-:\)\\\/\];=â€™#]","", processed_feature)   #remove remaning special characters

  processed_feature = processed_feature.lower() # make into lowercase

  processed_feature = re.sub(r'^rt\s+',' ', processed_feature) #remove rt (retweet)

  processed_features.append(processed_feature)

#after processing
processed_features[0]

"""#Wordclouds"""

wordcloud_df = pd.DataFrame(processed_features, y)
wordcloud_df.reset_index(level=0, inplace=True)
wordcloud_df.columns = ['sentiment', 'processed_tweets']

wordcloud_df

text = " ".join(i for i in wordcloud_df.processed_tweets)

wc = WordCloud(width=1600, height=800, max_words=500, colormap='gnuplot2', background_color='white')

wc.generate(text)
plt.figure(figsize=(12,10))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()

pos_tweets = wordcloud_df[wordcloud_df.sentiment == 1]
pos_string = []
for t in pos_tweets.processed_tweets:
  pos_string.append(t)
pos_string = pd.Series(pos_string).str.cat(sep=' ')

wordcloud = WordCloud(width=1600, height=800,max_font_size=200, max_words=200, colormap='gnuplot2', background_color='white' ).generate(pos_string)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

neg_tweets = wordcloud_df[wordcloud_df.sentiment == 0]
neg_string = []
for t in neg_tweets.processed_tweets:
    neg_string.append(t)
neg_string = pd.Series(neg_string).str.cat(sep=' ')

wordcloud = WordCloud(width=1600, height=800,max_font_size=200,max_words=200, colormap='gnuplot2', background_color='white').generate(neg_string)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""#vectorise"""

count_vect = CountVectorizer(max_features=500, stop_words = 'english')
tfidf_transf = TfidfTransformer()

feature_counts = count_vect.fit_transform(processed_features)

feature_counts.toarray()[0]

feature_tfidf = tfidf_transf.fit_transform(feature_counts)

feature_tfidf

feature_tfidf.toarray()[0]

X_train, X_test, y_train, y_test = train_test_split(feature_tfidf, y, random_state=42)

X_train

y_train.size

y_train

X_train.toarray()[0]

words = np.array(list(count_vect.vocabulary_.keys()))

words

"""#naive bayes (james)"""

nwds=500
cvect = CountVectorizer(max_features=nwds, stop_words = 'english')
X1 = cvect.fit_transform(processed_features)
X1 = X1.toarray()

#Number of times word w appears in revewis of class c
def count(w,c,X,y):
    tmp = X[:,w]
    tmp = tmp[y==c].sum()
    return tmp

counts = np.zeros((nwds,2))   
for c in range(2):
    for w in range(nwds):
        counts[w,c] = count(w,c,X1,y)

for w in range(10):
    print('{}, positive = {}, negative ={}'.format(words[w],counts[w,1],counts[w,0]))

#loglikelihoods wiht +1 smoothing

loglikes = np.zeros((nwds,2))

for c in range(2): #2 classes
    for w in range(nwds):
        loglikes[w,c] = np.log((counts[w,c]+1)/(counts[:,c]+1).sum() )

loglikes

#predictions
#if negatove loglike bigger than positive loglike, predict positive

ntxt = X1.shape[0]
preds=np.zeros(ntxt)
for i in range(ntxt):
    ll0=(X1[i]*loglikes[:,0]).sum()
    ll1=(X1[i]*loglikes[:,1]).sum()
    if ll1>ll0:
        preds[i]=1

#mean of error

1-np.abs(preds-y).mean()

print(classification_report(y, preds))

rats

rats = np.array([loglikes[w,1]-loglikes[w,0] for w in range(nwds)])
args = np.argsort(-rats)
for w in args:
    print(counts[w,1]/counts[w,0], words[w])

cmj = confusion_matrix(y, preds)
acj = accuracy_score(y, preds)

plt.figure(figsize=(9,9))
sns.heatmap(cmj, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(acj)
plt.title(all_sample_title, size = 15);

#my naive bayes

def train_naive_bayes(c, d): #returns log(p(c)) and log(p(w|c))

  for c in range(2):
    logprior[c] = np.log(len(c)/len(d))

  v = words

  for w in range(v):
    loglikes[w,c] = np.log((counts[w,c] + 1)/(counts[:,c] + 1).sum())

  return logprior, loglikes, v

def test_naive_bayes(testdoc, logprior, loglike, c, v): #returns best class

  for c in range(2):
    sum[c] = logprior[c]

  for i in testdoc:
    word = testdoc[i]
    
    if w in range(v):
      sum[c] = sum[c] + loglike[w, c]

  return max(sum[c])

"""#naive bayes model"""

X1_train, X1_test, y1_train, y1_test = train_test_split(feature_counts, labels, random_state=42)

X1_train.toarray()
X1_test.toarray()

X1_train.shape

y1_train.shape

mnb = MultinomialNB()

mnb.fit(X1_train.toarray(), y1_train)

y_pred = mnb.predict(X1_test.toarray())

print(classification_report(y1_test, y_pred))

cm = confusion_matrix(y1_test, y_pred)
ac = accuracy_score(y1_test, y_pred)

plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(ac)
plt.title(all_sample_title, size = 15);

"""#logistic regression model"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
#Scaling predictors and converting to a standard array
#Lasso regression (regularization parameter C)

Xs = scaler.fit_transform(X_train.toarray())

C_param_range = [0.01, 0.02, 0.04,0.05, 0.07, 0.1]
mean_scores = []

for C in C_param_range:
    clf = LogisticRegression(C=C,penalty='l1',solver='liblinear')
    scores = cross_val_score(clf, Xs, y_train, cv=10)
    mean_scores.append(scores.mean())
    print('C = ', C, ' mean score = ', scores.mean())

plt.figure(figsize=(12, 6))
plt.plot(C_param_range, mean_scores, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Cross-validation Scores')
plt.xlabel('C Parameter')
plt.ylabel('Mean score')

clf = LogisticRegression(C=0.05, penalty='l1',solver='liblinear', class_weight ='balanced')

clf.fit(X_train.toarray(), y_train)

scaled_X_test = scaler.fit_transform(X_test.toarray())

y_pred2 = clf.predict(scaled_X_test)

print(classification_report(y_test, y_pred2))

print(confusion_matrix(y_test, y_pred2))

cm2 = confusion_matrix(y_test, y_pred2)
ac2 = accuracy_score(y_test, y_pred2)

plt.figure(figsize=(9,9))
sns.heatmap(cm2, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(ac2)
plt.title(all_sample_title, size = 15);

"""#k_nearest neighbour"""

knn = KNeighborsClassifier(n_neighbors=13)

knn.fit(X_train, y_train)

knn_pred = knn.predict(X_test)

print(classification_report(y_test, knn_pred))

error = []

for i in range(1, 21):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error.append(np.mean(pred_i != y_test))

plt.figure(figsize=(12, 6))
plt.plot(range(1, 21), error, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')

cm3 = confusion_matrix(y_test, knn_pred)
ac3 = accuracy_score(y_test, knn_pred)

plt.figure(figsize=(9,9))
sns.heatmap(cm3, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(ac3)
plt.title(all_sample_title, size = 15);

"""#upsampling

random oversampling
"""

ROS = RandomOverSampler(random_state=42)

X_ROS, y_ROS = ROS.fit_sample(X1, y)

X_ROS.shape

X_train1, X_test1, y_train1, y_test1 = train_test_split(X_ROS, y_ROS, random_state=42)

X_train1.shape



y_train1.shape

X_test1.shape

ros_mnb = mnb.fit(X_train1, y_train1)

ros_clf = clf.fit(X_train1, y_train1)

ros_knn = knn.fit(X_train1, y_train1)

pred_mnb = ros_mnb.predict(X_test1)

pred_clf = ros_clf.predict(X_test1)

pred_knn = ros_knn.predict(X_test1)

print(classification_report(y_test1, pred_mnb))
print(classification_report(y_test1, pred_clf))
print(classification_report(y_test1, pred_knn))

n = 3

xmnb  = (0.71, 0.72)
xlr   = (0.63, 0.74)
xknn  = (0.80, 0.79)

barWidth = 0.25

bars1 = [0.71, 0.72]
bars2 = [0.63, 0.74]
bars3 = [0.80, 0.79]

r1 = np.arange(len(bars1))
r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r2]

plt.bar(r1, bars1, color='lightcoral', width=barWidth, edgecolor='white', label='Naive Bayes')
plt.bar(r2, bars2, color='lightgreen', width=barWidth, edgecolor='white', label='Logistic Regression')
plt.bar(r3, bars3, color='mediumpurple', width=barWidth, edgecolor='white', label='K-Nearest Neighbors')

plt.xlabel('F1 Scores Using Random Oversampling')
plt.xticks([r + barWidth for r in range(len(bars1))], ['Negative F1-Scores', 'Positive F1-Scores'])
 
# Create legend & Show graphic
plt.legend(bbox_to_anchor=(1.05, 1))
plt.show()

"""SMOTE"""

smt = SMOTE(random_state=42, k_neighbors=3)
X_SMOTE, y_SMOTE = smt.fit_sample(X1, y)

X_SMOTE[20000]

y_SMOTE.shape

X_train2, X_test2, y_train2, y_test2 = train_test_split(X_SMOTE, y_SMOTE, random_state=42)

smt_mnb = mnb.fit(X_train2, y_train2)

smt_lr = clf.fit(X_train2, y_train2)

smt_knn = knn.fit(X_train2, y_train2)

predsmt_mnb = smt_mnb.predict(X_test2)

predsmt_lr = smt_lr.predict(X_test2)

predsmt_knn = smt_knn.predict(X_test2)

print(classification_report(y_test2, predsmt_mnb))
print(classification_report(y_test2, predsmt_lr))
print(classification_report(y_test2, predsmt_knn))

barWidth = 0.25

bars1 = [0.70, 0.75]
bars2 = [0.70, 0.56]
bars3 = [0.73, 0.47]

r1 = np.arange(len(bars1))
r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r2]

plt.bar(r1, bars1, color='lightcoral', width=barWidth, edgecolor='white', label='Naive Bayes')
plt.bar(r2, bars2, color='lightgreen', width=barWidth, edgecolor='white', label='Logistic Regression')
plt.bar(r3, bars3, color='mediumpurple', width=barWidth, edgecolor='white', label='K-Nearest Neighbors')

plt.xlabel('F1 Scores Using SMOTE')
plt.xticks([r + barWidth for r in range(len(bars1))], ['Negative F1-Scores', 'Positive F1-Scores'])
 
# Create legend & Show graphic
plt.legend(bbox_to_anchor=(1.05, 1))
plt.show()

cm_ups = confusion_matrix(y_test, pred_smt)
ac_ups = accuracy_score(y_test, pred_smt)

plt.figure(figsize=(9,9))
sns.heatmap(cm_ups, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(ac_ups)
plt.title(all_sample_title, size = 15);

"""#other info"""

nwds=500

cvect = CountVectorizer(max_features=nwds, stop_words = 'english')
X1 = cvect.fit_transform(processed_features)
X1 = X1.toarray()

#Number of times word w appears in tweets of class c
def count(w, c, X, y):
    tmp = X[:,w]
    tmp = tmp[y==c].sum()
    return tmp

counts = np.zeros((nwds,2))   
for c in range(2):
    for w in range(nwds):
        counts[w,c] = count(w,c,X1,y)

pos_counts = []
neg_counts = []
words_list = []



for w in range(500):
  #make 3 lists to make a df
  words_list.append(words[w])
  pos_counts.append(counts[w,1])
  neg_counts.append(counts[w,0])
  print('{}, positive = {}, negative ={}'.format(words[w],counts[w,1],counts[w,0]))



term_freq = {'words' : words_list,
                'positive' : pos_counts,
                'negative' : neg_counts}

term_freq_df = pd.DataFrame(term_freq)

total_sent = term_freq_df['positive'] + term_freq_df['negative']

term_freq_df['total'] = total_sent

term_freq_df.sort_values(by='negative', ascending=False)

pos = np.arange(500)
plt.figure(figsize=(10,8))


plt.bar(pos, term_freq_df.sort_values(by='total', ascending=False)['total'][:500], align='center', alpha=0.5)

plt.plot((zipf))

plt.ylabel('Frequency')
plt.xlabel('Token')
plt.title('Word Frequency')
plt.show()

zipf = 1/(ranks*log(1.78*500))

from pylab import *
counts = term_freq_df.total
tokens = term_freq_df.index
ranks = arange(1, len(counts)+1)
indices = argsort(-counts)
frequencies = counts[indices]
plt.figure(figsize=(8,6))
plt.ylim(1,10**4)
plt.xlim(1,10**3)
loglog(ranks, frequencies, marker=".")
plt.plot([1,frequencies[0]],[frequencies[0],1],color='r')
title("Zipf plot for tweets tokens")
xlabel("Frequency rank of token")
ylabel("Absolute frequency of token")
grid(True)

#loglog plot
a=1

plt.figure(figsize=(10,8))
plt.scatter(np.log(pos), np.log(term_freq_df.sort_values(by='total', ascending=False)['total'][:500]))

plt.xlabel('Log of word frequency')
plt.ylabel('Log of number of such words')
plt.title('Power law for word frequencies')
plt.show()

words



"""#Compare knn times"""

import time

t = time.process_time()

knn.fit(X_train, y_train)

elapsed_time = (time.process_time - t)

from timeit import default_timer as timer

start = timer()

knn.predict(X_test)

end = timer()
print(end - start) # Time in seconds, e.g. 5.38091952400282

start = timer()

pred_knn = ros_knn.predict(X_test1)

end = timer()
print(end - start) # Time in seconds, e.g. 5.38091952400282

start = timer()

predsmt_knn = smt_knn.predict(X_test2)

end = timer()
print(end - start) # Time in seconds, e.g. 5.38091952400282

